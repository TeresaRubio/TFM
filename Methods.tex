La serie temporal anual en la que se centra este estudio, se divide en 3 bloques de muestras: saliva del sujeto A, intestino del sujeto A e intestino del sujeto B. Durante el periodo de estudio el donante A realiza un viaje al extranjero y el donante B sufre una salmonelosis. Las muestras son tomadas por los propios sujetos y posteriormente se secuencian en laboratorio mediante la plataforma Illumina GAIIx. Para el análisis completo, se llevan a cabo una serie de pasos en el siguiente orden: preprocesado de los datos, clasificación taxonómica, análisis de la variabilidad y estudio de interacciones. En el presente capítulo, se exponen los distintos materiales y métodos utilizados a lo largo del trabajo y la justificación de su elección.

\section{Preprocesado: FastQC, MultiQC y seq\_crumbs}
FastQC (\url{http://www.bioinformatics.babraham.ac.uk/projects/fastqc/}) es una herramienta de control de calidad para datos de secuenciación, de código abierto e implementada en Java. Acepta un fichero de entrada en distintos formatos (fastq, SAM o BAM) y produce un fichero de salida en formato HTML con gráficos y tablas que permiten evaluar los datos. Proporciona mucha información sobre una única muestra: estadísticas numéricas (codificación de calidad según la plataforma utilizada, número total de secuencias...), \textit{score} de calidad, contenido en GC, distribución de longitud de secuencias, etc. Esta herramienta permite detectar rápidamente cualquier problema que exista antes de realizar análisis posteriores.

MultiQC (\url{http://multiqc.info/}) es una herramienta de código abierto, implementada en Python y que da soporte a muchas herramientas bioinformáticas, entre ellas FastQC. Produce un reporte HTML muy parecido al anterior pero recoge la visualización de múltiples muestras en conjunto, permitiendo realizar comparaciones. También recopila estadísticas numéricas de cada muestra para ver cómo se comportan los datos.

seq\_crumbs (\url{https://bioinf.comav.upv.es/seq\_crumbs/}) es un software de código abierto, implementado en Python, que incluye utilidades para procesar secuencias. Toma un fichero de secuencias como entrada y crea un nuevo fichero de salida con las secuencias procesadas. Dentro de sus muchas funciones, caben destacar: filtrado de secuencias por calidad media, filtrado por longitud según un umbral máximo y mínimo, eliminación de regiones con baja calidad en los extremos (\textit{trimming}), etc.
%conversión de formatos, etc.

Se utilizan FastQC y MultiQC por la gran cantidad de información que producen, siendo especialmente útil en comprobar el comportamiento de la calidad de secuenciación. Por otro lado, se elige seq\_scrumbs porque incluye un \textit{script} específico para filtrar por calidad media bastante rápido y fácil de usar. Además, todas ellas se han seleccionado por ser de código abierto y estar disponibles para la comunidad tanto para su uso como para su adaptación, si fuera necesario.

\section{Clasificación taxonómica: qiime}
Qiime (\url{http://qiime.org/index.html}) \cite{Caporaso2010} son la siglas en inglés de \textit{Quantitative Insights Into Microbial Ecology}. Es un \textit{pipeline} bioinformático de código abierto para realizar análisis de microbiomas a partir de datos de secuenciación. Fue construido utilizando el lenguaje de programación Python con una implementación modular en forma de \textit{scripts} para poder usar cualquier punto dentro de su flujo de trabajo (figura \ref{qiime}) de manera independiente.

Qiime acepta ficheros de entrada en formato fastq, fasta+qual o sff. Incorpora su propio método de preprocesado que realiza el filtrado de \textit{reads} por calidad, longitud y el demultiplexado simultáneamente a partir de un fichero ``mapa'' con los metadatos (aunque no se usa en este trabajo). En este proyecto se utiliza qiime para la selección de OTUs y para la asignación de taxonomía, que son los dos siguientes pasos que incorpora el flujo de trabajo. Incluye, además, tres pasos adicionales con sus respectivas visualizaciones que tampoco se utilizan aquí: creación de árboles filogenéticos, estudio de diversidad $\alpha$ y $\beta$ y método de rarefacción.

Existen múltiples herramientas para realizar una clasificación taxonómica pero se prefiere qiime para poder reproducir y corroborar los resultados obtenidos por los autores del trabajo donde se obtienen los datos \cite{David2014}. Se generan diferencias taxonómicas a la hora de elegir una herramienta u otra, pero no son muy significativas y tanto qiime como mothur son métodos robustos.

\begin{figure}[]
   \centering
   \includegraphics[width=6in]{./Figuras/qiime.png}
   \caption[Flujo de trabajo de qiime.] {Flujo de trabajo de qiime. Se representan esquemáticamente todas las opciones de qiime. Cada una de ellas las realiza un \textit{script} diferente, de tal manera que puede iniciarse el análisis en cualquier punto del flujo de trabajo.}
   \label{qiime}
\end{figure}

\subsection [Selección de OTUs] {\textbf{Selección de OTUs}} 
OTU (del inglés \textit{Operational Taxonomic Unit}) es una unidad taxonómica operativa, es decir, una unidad de clasificación elegida por el investigador para individualizar los objetos de su estudio sin juzgar si se corresponden a una entidad biológica particular. Se aplica cuando se tienen datos de secuencias de ADN o datos morfológicos. Puede considerarse OTU un individuo, una población, una especie o cualquier otro taxón. Qiime ofrece tres estrategias de selección diferentes para este paso:

	\begin{itemize}
	\addtolength{\itemsep}{-3mm}  % separacion entre items
	\item \textit{\underline{Closed-reference}}: Las lecturas son agrupadas contra una colección de secuencias referencia y las que no agrupan son excluidas del análisis. Es el método más rápido, al ser muy paralelizable, y se obtienen mejores taxonomías porque son OTUs definidas previamente. Sin embargo, no permite detectar nuevas OTUs así que depende mucho de lo bien caracterizada que esté la base de datos. Los métodos de agrupación que se pueden utilizar son: \textit{blast, uclust} y \textit{usearch}.
	\item \textit{\underline{De novo}}: Las lecturas se agrupan por similitud unas contra otras, sin ningún tipo de referencia externa. El beneficio es que todas las \textit{reads} son agrupadas pero no es paralelizable por lo que sería un proceso muy lento para grandes sets de datos. Los métodos de agrupación son: \textit{uclust} y \textit{usearch}.
	\item \textit{\underline{Open-reference}}: Las lecturas son agrupadas contra la referencia y las que no se encuentran en la referencia, son agrupadas posteriormente \textit{de novo}. Presenta la ventaja de que todas las \textit{reads} quedan agrupadas y además es paralelizable la mitad del proceso. Suele ser la estrategia preferida aunque no es recomendable utilizarla en datos con pocas referencias porque el proceso puede tardar días en completarse. Los métodos de agrupación son: \textit{cd-hit, mothur, prefix/suffix, trie, uclust} y \textit{usearch}.
	\end{itemize}



En este proyecto se selecciona \textit{open-reference} como estrategia de selección para que todas las lecturas queden agrupadas incluso aunque no se encuentren en la base de datos, ya que nos interesa obtener toda la información posible de los datos. Al final del proceso, se obtiene una tabla cuya primera columna es el identificador de OTU y la segunda columna son los conteos, que pueden generarse en valor absoluto o relativo.

\paragraph[Greengenes]{\textit{Greengenes (\url{http://greengenes.secondgenome.com})}}
Es la base de datos que se elige como referencia en este caso. Contiene taxonomía de 16S de calidad controlada, basada en una filogenia \textit{de novo} que proporciona conjuntos de OTUs estándar. Está bajo la licencia Creative Commons BY-SA 3.0. Incluye los siguientes niveles de taxonomía: 
\begin{itemize}
\addtolength{\itemsep}{-3mm}  % separacion entre items
\item Nivel 1 = Reino (por ejemplo, \textit{Bacteria}), 
\item Nivel 2 = Filo (por ejemplo, \textit{Firmicutes}), 
\item Nivel 3 = Clase (por ejemplo, \textit{Bacilli}), 
\item Nivel 4 = Orden (por ejemplo, \textit{Lactobacillales}), 
\item Nivel 5 = Familia (por ejemplo, \textit{Streptococcaceae}), 
\item Nivel 6 = Género (por ejemplo, \textit{Streptococcus}), 
\item Nivel 7 = Especies (por ejemplo, \textit{pneumoniae}).
\end{itemize}
Un ejemplo del formato sería:

\begin{center} \textbf{\textit{k\_\_Bacteria; p\_\_Firmicutes; c\_\_Bacilli; o\_\_Lactobacillales; f\_\_Streptococcaceae; g\_\_Streptococcus; s\_\_pneumoniae}} \end{center}

\paragraph [Uclust]{\textit{Uclust (\url{http://drive5.com/usearch/manual/uclust_algo.html})}}
Es el método de agrupación utilizado en estos datos. Es un algoritmo diseñado para agrupar secuencias de nucleótidos o aminoácidos en base a su similitud \cite{Edgar2010}. Cada grupo o \textit{cluster} está definido por una secuencia representativa conocida como ``centroide". Sigue dos criterios de agrupamiento simples, con respecto a un umbral de similitud (T) dado: (1) todas las secuencias dentro de un \textit{cluster} tienen similitud $>$= T con la secuencia centroide y (2) todos los centroides tienen similitud $<$ T entre ellos. Hay que tener en cuenta que una secuencia puede coincidir en similitud con dos centroides diferentes. Idealmente, se asignará al centroide más cercano, pero puede haber dos o más centroides a la misma distancia, en cuyo caso la asignación de \textit{cluster} es ambigua y se debe tomar un criterio de elección arbitrario. La similitud se calcula utilizando alineamiento global. Además, se trata de un algoritmo voraz (también conocido como \textit{greedy}) que es aquél que elige la opción óptima en cada paso local esperando llegar a una solución óptima global, por lo que es importante el orden en que van entrando las secuencias. Si la secuencia entrante coincide con un centroide existente, se asigna a ese grupo y si no coincide, se convierte en el centroide de un nuevo grupo. Esto significa que las secuencias deben estar ordenadas para que los centroides más adecuados tienden a aparecer más temprano. Dado que las lecturas más abundantes tienen más probabilidades de ser secuencias de amplicones correctas, y por tanto son más probables de ser verdaderas secuencias biológicas, considera las secuencias de entrada en orden de disminución de la abundancia.

El motivo por el que se selecciona el método \textit{uclust} y la base de datos \textit{greengenes} es porque se pretende reproducir lo que los autores de los datos obtienen originalmente y ellos emplean estas opciones. Tanto los métodos como las bases de datos actuales están bastante optimizados y decantarse por uno u otro implica obtener diferencias, aunque son poco significativas.

\subsection [Asignación taxonómica]{\textbf{Asignación taxonómica}}
Una vez creada la tabla OTU, qiime permite asignar una taxonomía a cada secuencia representativa. Actualmente los métodos implementados son BLAST, clasificador RDP, RTAX, mothur y uclust. Después, se realiza un resumen de la representación de los grupos taxonómicos dentro de cada muestra según un nivel elegido por el usuario. Ese nivel depende del formato que se devuelve desde el paso de la asignación de taxonomía (nivel 1 a 7).  La salida de este proceso es una tabla donde la primera columna es la taxonomía y en la segunda columna se mantienen los conteos en valor absoluto o relativo.



\section{Análisis de variabilidad: complexCruncher} 
\vspace*{-1mm}
Es un software de código abierto implementado en Python que sirve para el estudio de la variabilidad en series temporales \cite{ccruncher2017}. Acepta como \textit{input} ficheros excel (en formato xlsx) o ficheros de texto, y genera como \textit{output} una serie de gráficos (eps, svg, png, pdf o ps) y tablas (tex o xlsx). Se puede utilizar en modo automático, que analiza en paralelo todos los conjuntos de datos de entradas, o interactivo, que genera los resultados que el usuario pide. En los siguientes apartados se especifica la forma de generar cada resultado.


\subsection[Regresión lineal y exponencial]{\textbf{Regresión lineal y exponencial}}
Primero se comprueba si se ajusta a una recta de regresión la desviación estándar de los datos frente a su media. La idea es que los valores observados se sitúen lo más cerca posible de la recta de ajuste, minimizando sus distancias a la misma. Esas distancias se denominan errores residuales o, simplemente, residuos. Para realizar el ajuste, se suman todos los cuadrados de los errores residuales para obtener un solo error que se conoce como suma de los errores al cuadrado (SSE -- del inglés ``Sum of Squares Error'') y se elige la recta con el valor SSE más pequeño.

En ocasiones los datos no se ajustan a una recta sino a una curva exponencial de la forma $y = A\cdot r^x$. En estos casos, se convierte la curva exponencial en recta por medio de logaritmos y se aplica el ajuste visto anteriormente. Aplicando propiedades de logaritmos quedaría de la forma:
\begin{equation}
\log y = \log A + x \cdot \log r
\end{equation}

donde la pendiente es $\log r$ y la intersección con el eje de ordenadas es $\log A$.


\subsection[Ley de potencia $x$-ponderada]{\textbf{Ley de potencia $x$-ponderada}}
Cuando se ajusta la ley de potencia de desviación estándar frente a la media, hay que tener en cuenta que cada media tiene incertidumbre y se puede estimar, para un tamaño de muestra $n$, por el error estándar de la media (SEM -- del inglés \textit{Standard Error of the Mean}). En este caso, las incertidumbres afectan a la variable independiente, por lo que el ajuste no es tan trivial como un ajuste y-ponderado, donde las incertidumbres afectan a la variable dependiente. Un método estándar para realizar este ajuste es (1) invertir las variables antes de aplicar los pesos, (2) realizar el ajuste ponderado, y (3) revertir la inversión. Este método es determinista, pero la solución aproximada empeora con coeficientes de determinación más pequeños. Para superar esa limitación, se desarrolla un método estocástico con una estrategia de tipo \textit{bootstrap} que evita la inversión y es aplicable independientemente del coeficiente de determinación.

La idea básica del \textit{bootstrap} es que la inferencia sobre una población de datos muestreados puede ser modelada mediante un nuevo muestreo de los datos y realizando la inferencia a partir de datos remuestreados. Para adaptar esta idea general al problema aquí descrito, se realizan múltiples replicaciones donde se remuestrea la matriz de datos $x$ utilizando su matriz de errores. Es decir, se calcula cada vez una nueva matriz de datos $x$ sobre la base de $x^*_i = x_i + v_i$, donde $v_i$ es una variable aleatoria gaussiana con media $\mu_i=0$ y desviación estándar $\sigma_i=\mathrm{SEM}_i$. En cada repetición, se realiza un ajuste completo de la ley de potencia no ponderada. Los parámetros de la x-ponderación se estiman promediando a través de todos los ajustes de repetición realizados, y sus errores se determinan mediante el cálculo de la desviación estándar para todos los ajustes.

\subsection[Estandarización]{\textbf{Estandarización}}
Sirve para visualizar varios estudios en un diagrama compartido con unidades de desviación estándar de los parámetros Taylor en sus ejes. Para ello, se estandarizan $V$ y $\beta$ utilizando el grupo de sujetos sanos de cada estudio individualmente.

%Para mostrar varios estudios bajo ejes comunes, se estandarizan los parámetros de Taylor utilizando el grupo de sujetos sanos de cada estudio individualmente. Con este enfoque, todos los estudios se pueden visualizar en un diagrama compartido con unidades de desviación estándar de los parámetros Taylor en sus ejes.

Para el parámetro $V$, la estimación de la media ($\widehat{V}$) del grupo de sanos, compuesta por $h$ individuos, es:
\vspace*{-1.5mm}
\begin{equation}
\widehat{V} = \frac{1}{W_1}\sum_{i=1}^h V_i \omega_i=\sum_{i=1}^h V_i \omega_i
\end{equation}

con $W_1=\sum_i^h \omega_i=1$, donde $\omega_i$ son los pesos normalizados calculados como:
\vspace*{-1.5mm}
\begin{equation}
\omega_i = \frac{\frac{1}{\sigma^2_{V_i}}}{\sum_i^h\frac{1}{\sigma^2_{V_i}}}
\end{equation}

donde $\sigma_{V_i}$ es una estimación de la incertidumbre en $V_i$ obtenida junto con $V_i$ de la ley de potencia $x$-ponderada (descrita en el apartado anterior) para sujetos sanos.\\


Del mismo modo, la estimación de la desviación estándar para la población sana ($\widehat{\sigma}_V$) es:
\begin{equation}
\widehat{\sigma}_V = \sqrt{\frac{1}{W_1-\frac{W_2}{W_1}}\sum_{i=1}^h\left[\omega_i\left(V_i-\hat{V}\right)^2\right]}
\end{equation}
con $W_2=\sum_i^h \omega_i^2$, que finalmente queda como:
\begin{equation}
\widehat{\sigma}_V = \sqrt{\frac{1}{1-\sum_i^h \omega_i^2}\sum_{i=1}^h\left[\omega_i\left(V_i-\hat{V}\right)^2\right]}
\end{equation}


% Esto está relacionado con el cálculo de la "healthy zone".

\subsection[RSI y medidas de variabilidad]{\textbf{RSI y medidas de variabilidad}}
ComplexCruncher genera unas matrices de rango para los 50 taxones más abundantes (figuras \ref{rank_saliva}, \ref{rank_stoolA} y \ref{rank_stoolB}) que muestran el puesto de un taxón en el ranking de abundancia. En la parte derecha de estas matrices puede observarse una barra que mide el índice de estabilidad de rango (RSI -- siglas en inglés de \textit{Rank Stability Index}) en porcentaje. RSI puede oscilar entre 0 y 1, siendo estrictamente 1 para un elemento cuyo rango nunca cambia con el tiempo y 0 para un elemento cuyo rango oscila entre los extremos. Por tanto, RSI se calcula para cada elemento como:

\begin{equation}
{\rm RSI} = \left(1-\frac{\text{saltos de rango reales}}{\text{saltos de rango posibles}}\right)^p = \left(1-\frac{D}{(N-1)(t-1)}\right)^p
\end{equation}

donde $D$ es el número total de saltos de rango dados por el elemento estudiado, $N$ es el número de elementos que han sido clasificados, y $t$ es el número de muestras temporales. El índice de potencia, $p = 4$, se elige arbitrariamente para aumentar la resolución en la región estable.

Finalmente, bajo las matrices de rango, hay un gráfico con dos medidas relevantes para la variabilidad del rango a lo largo del tiempo. Por un lado, la variabilidad de rango (RV -- siglas en inglés de \textit{Rank variability}) se calcula como un promedio para todos los taxones del valor absoluto de la resta entre el rango de cada taxón en el tiempo que se calcula y el rango global de cada taxón. Y por otro lado, las diferencias de variabilidad (DV -- del inglés \textit{Differences Variability}) se calculan como un promedio para todos los taxones del valor absoluto de la resta entre el rango de cada taxón en el tiempo que se calcula y el rango que tiene en el tiempo anterior.

\vspace*{5mm}
\section[Estudio de interacciones]{Estudio de interacciones}

\subsection[Coeficiente de correlación de Pearson]{\textbf{Coeficiente de correlación de Pearson}}
En muchos trabajos se utilizan las correlaciones como medida de interacción entre taxones, de tal manera que si dos géneros aparecen y desaparecen de forma similar a lo largo del tiempo quiere decir que están interaccionando. Existen diversos coeficientes que miden el grado de correlación, adaptados a la naturaleza de los datos, pero el más conocido es el coeficiente de correlación de Pearson y es el que se aplica a los datos de abundancia absoluta de este estudio. \clearpage Este coeficiente mide el grado de relación lineal entre dos variables cuantitativas ($x$ e $y$) sobre una población y se calcula con la siguiente expresión:
\begin{equation}
\rho_{x,y} = \frac{\sigma_{xy}} {\sigma_x \cdot \sigma_y}
\end{equation}

donde $\sigma_{xy}$ es la covarianza de $(x,y)$, $\sigma_x$ es la desviación típica de $x$ y $\sigma_y$ es la desviación típica de $y$.

El resultado numérico fluctua entre el rango $[-1, +1]$. Una correlación de $+1$ significa que existe una relación lineal directa perfecta (positiva) entre las dos variables estudiadas. Una correlación de $-1$ significa es una relación lineal inversa perfecta (negativa). Y una correlación de $0$ se interpreta como que no existe una relación lineal (pero pueden darse otras).


\subsection{Método de búsqueda de comportamientos}
Se pretende agrupar taxones en grupos que reflejen su comportamiento frente a una perturbación (viaje o salmonelosis) para reordenar la matriz de correlaciones. Para ello, se ha diseñado un método sencillo dividiendo las tablas de abundancia relativa en 3 periodos: antes ($a$), durante ($d$) y tras ($r$) el punto en cuestión. Para cada taxón, se calcula la mediana de su abundancia en cada uno de estos periodos ($M_a$, $M_d$, $M_t$). Por último, se calculan los incrementos $\Delta M_1 = M_a - M_d$ y $\Delta M_2 = M_t - M_d$. Si $\Delta M_1$ es positivo, quiere decir que la abundancia ha disminuído con la perturbación y si el incremento es negativo, quiere decir que ha aumentado. En $\Delta M_2$ se da el caso contrario, valores positivos indican disminución de abundancia y valores negativos indican aumento. Además, se escoge un valor de $0.01$ para que el incremento se considere significativamente grande como para que haya cambio en la abundancia. En la tabla \ref{grupos_comportamiento}, se especifican los 7 grupos que se han considerado.

\begin{table}[h]
\centering
\resizebox{15cm}{!} {
\begin{tabular}{| p{3cm}| p{4cm} | c | p{6cm} |}
\hline
Grupo & Valores & Comportamiento & Descripción\\
\hline \hline
\textbf{\textcolor{green}{Grupo 1}} & $\Delta M_1 < 0.01$ y \newline $\Delta M_2 > 0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{1mm} \includegraphics[width=.9\textwidth]{Figuras/g1.png} \end{minipage} & Aumenta tras la perturbación.\\ \hline %\cline{1-3}
\textbf{\textcolor{red}{Grupo 2}} & $\Delta M_1 > 0.01$ y \newline $\Delta M_2 > 0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{2mm} \includegraphics[width=.9\textwidth]{Figuras/g2.png} \end{minipage} & Disminuye durante la perturbación y recupera el estado inicial.\\ \hline
\textbf{\textcolor{orange}{Grupo 3}} & $\Delta M_1 > 0.01$ y \newline $\Delta M_2 < 0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{1mm}\includegraphics[width=.9\textwidth]{Figuras/g3.png} \end{minipage} & Disminuye con la perturbación.\\ \hline
\textbf{\textcolor{pink}{Grupo 4}} & $\Delta M_1 < -0.01$ y \newline $\Delta M_2 > -0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{2mm} \includegraphics[width=.9\textwidth]{Figuras/g4.png} \end{minipage} & Aumenta con la perturbación.\\ \hline
\textbf{\textcolor{Plum}{Grupo 5}} & $\Delta M_1 < - 0.01$ y \newline $\Delta M_2 > -0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{2mm} \includegraphics[width=.9\textwidth]{Figuras/g5.png} \end{minipage} & Aumenta durante la perturbación y recupera el estado inicial.\\ \hline
\textbf{\textcolor{SkyBlue}{Grupo 6}} & $\Delta M_1 > -0.01$ y \newline $\Delta M_2 < -0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{1.5mm} \includegraphics[width=.9\textwidth]{Figuras/g6.png} \end{minipage} & Disminuye tras la perturbación.\\ \hline
\textbf{Grupo 7} & $-0.01 < \Delta M_1 < 0.01$ y \newline $-0.01 < \Delta M_2 < 0.01$ \vspace*{3mm} & \begin{minipage}{2.5cm} \vspace*{3mm} \includegraphics[width=.9\textwidth]{Figuras/g7.png} \end{minipage} & Sin variación o con variación independiente de la perturbación.\\ \hline
\end{tabular}
}
\caption[Tabla de grupos de comportamiento.]{Tabla de grupos de comportamiento de los microorganismos encontrados tras una perturbación. La primera columna recoge los nombres de los grupos y su color identificativo, la segunda columna incluye los valores de incremento de la mediana que debe tener cada taxón para pertenecer al grupo, la tercera columna describe el comportamiento del grupo y la última especifica la descripción del comportamiento.}
\label{grupos_comportamiento}
\end{table}


\subsection{Aproximación para obtener interacciones: LIMITS} 
Trabajos más actuales como el de Fisher \textit{et al.} \cite{Fisher2014} han cuestionado que las correlaciones midan interacciones reales y, además, han desarrollado un nuevo método que se ha aplicado a los datos para comparar frente a correlaciones. El algoritmo se denomina LIMITS (siglas de \textit{Learning Interactions from MIcrobial Time Series}) y utiliza una regresión lineal con agregación \textit{bootstrap} para inferir el modelo Lotka-Volterra de tiempo discreto (dLV) en la dinámica de los microorganismos. 
\newpage
El modelo dLV, también conocido como modelo de Ricker, es un modelo clásico de población discreta que relaciona la abundancia de una especie $i$ a tiempo $t+1$ ($x_i(t+1)$) con la abundancia de todas las especies del ecosistema en el tiempo $t$ ($\vec{x}=\left\lbrace x_1(t),...,x_N(t)\right\rbrace$). Las interacciones  se calculan a través del coeficiente de interacción, $c_{ij}$, que describe la influencia que la especie $j$ tiene sobre la abundancia de la especie $i$. La dinámica se modela con la ecuación:
\begin{equation}
x_i(t+1) = \eta_i(t) \cdot x_i(t) \cdot \exp\left(\sum_{j}c_{ij}(x_{j}(t) - <x_j>)\right)
\end{equation}
donde $\eta_i(t)$ es el ruido multiplicativo con distribución log-normal y $<x_j>$ es la abundancia de equilibrio de las especies $j$ y se establece por la capacidad de carga del entorno. Aplicando logaritmos se pueden obtener los coeficientes de interacción.

\clearpage
El algoritmo LIMITS está implementado en Mathematica (Wolfram Research, Inc.). Intenta inferir la matriz de interacciones entre microorganismos a partir de la abundancia absoluta de los microbios en el ecosistema. El procedimiento utiliza regresión por pasos y \textit{bootstrap} que están esquematizados en la figura \ref{LIMITS}. La matriz se inicializa con valores en la diagonal ($c_{ii}$) distintos de cero porque se sabe que cada especie tiene que interaccionar consigo misma. En cada subsecuente iteración, se añade una interacción adicional $c_{ij}$ al modelo escaneando el resto de especies y eligiendo la que produce el menor error en el grupo test.

\begin{figure}[h]
   \centering
   \includegraphics[width=6in]{./Figuras/LIMITS.png}
   \caption[Procedimiento de LIMITS.] {Procedimiento de LIMITS. a) Utiliza una regresión por pasos, donde las interacciones se añaden secuencialmente a la regresión si su inclusión reduce el error de predicción por debajo de un umbral predefinido. b) El error se evalúa por agregación bootstrap, que consiste en dividir los datos de forma aleatoria en dos conjuntos: uno de entrenamiento utilizado para la regresión y otro de sondeo para evaluar el error. c) La regresión por pasos se aplica repetidas veces para construir varios conjuntos de entrenamiento que finalmente se simplifican en uno solo aplicando la mediana a todos ellos.}
   \label{LIMITS}
\end{figure}

